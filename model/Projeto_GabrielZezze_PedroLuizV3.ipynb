{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-67506f34-d151-448b-95ba-10aaae723fd7",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "# Drawing Guesser \n",
    "## Pedro Luiz & Gabriel Zezze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-582329ba-9020-41ec-a821-d4d8196c212c",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### O dataset\n",
    "O drawing guesser se baseia no [dataset de desenhos dispopnibilizados pelo Google](https://github.com/googlecreativelab/quickdraw-dataset) gerado através do jogo [QuickDraw](https://quickdraw.withgoogle.com/).\n",
    "\n",
    "O dataset completo consiste de 150 mil desenhos que são imagens bitmap de 28x28 para cada categoria sendo que há 345 categorias disponibilizadas.\n",
    "\n",
    "O intuito do drawing guesser é simular o proprio QuickDraw, ou seja, fazer classificações do desenho enquanto o jogador desenha, porém com um número limitado de catogrias (x) e uma rede neural própria treinada a partir do dataset disponibilizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00002-d9d5ef69-0f53-4666-b723-9828344b40a2",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### O problema\n",
    "\n",
    "Este é um problema de classifição multiclasse dos desenhos dispoinibilizados no dataset. Para realizar essa classificação vamos utilizar uma rede convulacional que será explicada detalhadamente abaixo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "00000-afc88602-3533-45b6-bdac-b1ea73af36f9",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "#IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ujson as json\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout, BatchNormalization\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "00001-747ea66f-189b-4956-9891-dff7582e5f18",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "#DECLARE CONSTANTS\n",
    "IMG_HEIGHT = 28\n",
    "IMG_WIDTH = 28\n",
    "NUM_FILES = 35000\n",
    "RF_NUM_FILES = 10000\n",
    "N_EPOCHS = 300\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-6d2f6b48-249f-4fa4-918a-7dae7bbf8bee",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Extração de dados\n",
    "\n",
    "O dataset utilizado disponibiliza arquivos do tipo numpy bit array, que quando lidos com a função `numpy.load` retorna um numpy array. Contudo, esse array possui apenas uma dimensão. Dessa forma, é necessário fazer o reshape de cada imagem lida para se tornar uma matriz 28x28. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cell_id": "00002-e96aecac-cd38-4d76-9a78-91033a447e41",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 539,
    "execution_start": 1623245615866,
    "source_hash": "7b74984"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/book.npy', './data/bus.npy', './data/camel.npy', './data/apple.npy', './data/airplane.npy', './data/cactus.npy', './data/bush.npy', './data/butterfly.npy', './data/calculator.npy', './data/bird.npy', './data/ambulance.npy', './data/cake.npy', './data/beach.npy', './data/axe.npy', './data/banana.npy', './data/calendar.npy', './data/alarm clock.npy', './data/basketball.npy', './data/bed.npy', './data/barn.npy']\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "#GETTING ALL FILE NAMES\n",
    "data_files = os.listdir('./data/')\n",
    "data_files = [f'./data/{file}' for file in data_files]\n",
    "if data_files.count('./data/.DS_Store') > 0:\n",
    "    data_files.remove('./data/.DS_Store')\n",
    "N_CATEGORIES = len(data_files)\n",
    "\n",
    "# data_files = [\n",
    "#     '../data/book.npy',\n",
    "#     '../data/bus.npy',\n",
    "#     '../data/camel.npy',\n",
    "#     '../data/apple.npy', \n",
    "#     '../data/airplane.npy',\n",
    "#     '../data/cactus.npy',\n",
    "#     '../data/bush.npy',\n",
    "#     '../data/butterfly.npy',\n",
    "#     '../data/calculator.npy',\n",
    "#     '../data/bird.npy',\n",
    "#     '../data/ambulance.npy',\n",
    "#     '../data/cake.npy',\n",
    "#     '../data/beach.npy',\n",
    "#     '../data/axe.npy',\n",
    "#     '../data/banana.npy',\n",
    "#     '../data/calendar.npy',\n",
    "#     '../data/alarm clock.npy',\n",
    "#     '../data/basketball.npy',\n",
    "#     '../data/bed.npy',\n",
    "#     '../data/barn.npy'\n",
    "# ]\n",
    "\n",
    "print(data_files)\n",
    "print(N_CATEGORIES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cell_id": "00003-51ec2a30-1091-460b-9666-d681a3415ff4",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "#READING FILES AND POPULATING TOTAL X AND Y\n",
    "all_drawings = []\n",
    "all_categories = []\n",
    "\n",
    "for idx, file in enumerate(data_files):\n",
    "    data = np.load(file)[:NUM_FILES]\n",
    "    for d in data:\n",
    "        reshaped_img = np.array(np.reshape(d, (-1, 28))).astype(np.float32)\n",
    "        \n",
    "        all_drawings.append(reshaped_img)\n",
    "        all_categories.append(idx)\n",
    "\n",
    "all_drawings = np.array(all_drawings)\n",
    "all_categories = np.array(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00008-d97d4154-4ce2-4406-bd2f-6928226375e8",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Selecao de dados entre dados de treinamento e teste\n",
    "\n",
    "Antes de qualquer treinamento ou teste em cima dos dados é necessário separá-los em dados de treinamento e teste sendo 20% dos dados seprados para teste e o restante para treinamento. \n",
    "\n",
    "Após essa separação os dados de trinamento serão divididos mais uma vez, sendo que 80% deste dados serao para treinamento do modelo e 20% para validação do treinamento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00004-126006f2-6480-49d7-85f7-8e155808e851",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(all_drawings, all_categories, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00005-b5116cb9-56d0-4654-a7c8-d59d1a24fe4c",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## Estabelecendo uma base\n",
    "Antes de iniciar a criação da rede convolucional, precisamos de uma base para saber se nosso modelo faz classificaçõeses melhores que modelos aleatórios. Para isso vamos utilizar o classificador Random Forrest para estabelecer um mínimo de performance para nosso modelo convolucional. Dessa forma, se a rede concolucional criada possuir desempenho similar ao classificador aleatório, não podemos dizer que obtivemos qualquer sucesso.\n",
    "\n",
    "Como o classificador Random forrest recebe dados de treinamento de uma forma diferente (Vetores de no máximo duas dimensões) que uma rede convolucional, precisamos mudar o formato das features para treiná-lo.\n",
    "Após treinar o classificador Random forest usamos o método `cross_val_score` para separarmos os dados de treinamento em 5 divisoes. Esse método combina 4 divisões para treinamento e a divisão restante para validação. Por fim, obtivemos os seguintes valores de acurácia de validação:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-7582ac69-3e78-41c1-beec-cfd7c433f9c8",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf_drawings = np.array([all_drawings[i:i + RF_NUM_FILES] for i in range(0, len(all_drawings), NUM_FILES)])\n",
    "rf_drawings = np.concatenate([draw for draw in rf_drawings])\n",
    "\n",
    "rf_categories = np.array([all_categories[i:i + RF_NUM_FILES] for i in range(0, len(all_categories), NUM_FILES)])\n",
    "rf_categories = np.concatenate([cat for cat in rf_categories])\n",
    "\n",
    "X_rf_train_full, X_rf_test, y_rf_train_full, y_rf_test = train_test_split(rf_drawings, rf_categories, test_size=0.2, random_state=RANDOM_SEED)\n",
    "\n",
    "random_forest_X_train = np.array([x.flatten() for x in X_rf_train_full])\n",
    "random_forrest_clf = RandomForestClassifier(random_state=RANDOM_SEED, n_jobs=-1)\n",
    "\n",
    "cv_rf_score = cross_val_score(random_forrest_clf, random_forest_X_train, y_rf_train_full, cv=5, n_jobs=-1)\n",
    "for idx, val in enumerate(cv_rf_score):\n",
    "    print(f\"{idx+1}:  {round(val*100, 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00013-9039431f-d71f-40e3-a519-9557a0b76e45",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Montagem da rede convolucional\n",
    "\n",
    "O modelo criado possui 3 conjuntos de camadas convolucionais seguidas de um max pooling. Os filtros da camada convolucional começam em 8 e vão dobrando, já para o kernel initializer foi utilizado o padrão que é o `glorot_uniform` de tamanho 3x3. Essas primeiras camadas tem o objetivo de destacar as features únicas(detalhes) de cada categoria. Em seguida é realizado o Flatten para transformar todas as camadas em uma array 1D e um Batch Normalization para padronizar os valores, ou seja, centralizar sua média em 0 e o desvio padrão em 1. Também foi adicionada uma camada de Dropout para evitar o overfitting. Na sequência, os inputs normalizados  são passados para uma camada Densa, a qual recebe dois parametros, número de neurônios e função de ativação que sera usada, assim a camada densa recebe as features, multiplica pelos respectivos pesos e soma esta multiplicação para finalmente passar pela funcao de ativação. Em seguida é aplicada mais uma batch normalization e um dropout. Por fim, é aplicada mais uma camada Dense com units = número de categorias, com a função de ativação soft max, para obter o resultado da classificação.\n",
    "\n",
    "#### Entradas e saídas das camadas\n",
    "```\n",
    "(28, 28, 1) -> Primeira camada Conv2D -> (28, 28, 8)\n",
    "(28, 28, 8) -> Primerira camada Maxpooling2D -> (14, 14, 8)\n",
    "(14, 14, 8) -> Segunda camada Conv2D -> (14, 14, 16)\n",
    "(14, 14, 16) -> Segunda camada Maxpooling2D -> (7, 7, 16)\n",
    "(7, 7, 16) -> Terceira camada Conv2D -> (7, 7, 32)\n",
    "(7, 7, 32) -> Terceira camada Maxpooling2D -> (4, 4, 32)\n",
    "(4, 4, 32) -> Flatten -> (512)\n",
    "(512) -> Dense -> (30)\n",
    "(30) -> Dense -> (N)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00008-ab0e3ec3-8cf3-4341-895a-36d0e5291014",
    "deepnote_cell_type": "code"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-0c55aa62f37b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mslice_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mslice_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_valid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mslice_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_full' is not defined"
     ]
    }
   ],
   "source": [
    "slice_index = int(len(X_train_full)*0.8)\n",
    "\n",
    "X_train = X_train_full[:slice_index][..., np.newaxis]\n",
    "X_valid = X_train_full[slice_index:][..., np.newaxis]\n",
    "\n",
    "y_train = y_train_full[:slice_index][..., np.newaxis]\n",
    "y_valid = y_train_full[slice_index:][..., np.newaxis]\n",
    "\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00009-c41bd223-23d0-402f-90f0-2fdf355e7eab",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    Conv2D(filters=8, kernel_size=3,padding=\"same\", activation=\"relu\", input_shape=(IMG_HEIGHT,IMG_WIDTH,1)),\n",
    "    MaxPool2D(pool_size=2),\n",
    "\n",
    "    Conv2D(filters=16, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=2),\n",
    "\n",
    "    Conv2D(filters=32, kernel_size=3, padding=\"same\", activation=\"relu\"),\n",
    "    MaxPool2D(pool_size=2),\n",
    "    \n",
    "    Flatten(),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5, seed=RANDOM_SEED),\n",
    "    Dense(units=30,activation=\"relu\"),\n",
    "    \n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5, seed=RANDOM_SEED),\n",
    "    Dense(units=N_CATEGORIES, activation=\"softmax\"),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00010-798a5aaa-0fb4-41e4-baf3-2b1714d92cfa",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=1e-4), \n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False), \n",
    "    metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00011-5b757315-6ffb-47bb-8256-acb849bf826b",
    "deepnote_cell_type": "code",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_train,y_train,epochs = N_EPOCHS, validation_data=(X_valid, y_valid), batch_size=264)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('./trained_models/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00019-a806faee-b006-45c2-8e7c-472d4464102a",
    "deepnote_cell_type": "markdown",
    "tags": []
   },
   "source": [
    "### Análise do treinamento\n",
    "Pode-se perceber pelos gráficos 1 e 2 que o modelo apresenta uma melhora na acurácia e no erro a cada iteração. Além disso, a validação acompanha essa melhora do treinamento, ou seja, o modelo apresenta bons resultados para o que ele está sendo treinado e também para dados genéricos(dados que nao estavam presente no treinamento), eliminando a possibilidade de overfitting. Por fim, a acurácia de validação do treinamento e o erro dessa validação são melhores que o próprio treinamento, visto que há duas camadas de dropout durante o treinamento, que eliminam aleatóriamente alguns dados de entrada, mas mantendo a soma total das entradas. Dessa forma, o modelo não se acomoda com os dados de treinamento e é obrigado a ser capaz de generalizar (evitando overfitting)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00013-7ef1be91-0b0f-4278-b119-046ea892e352",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = range(N_EPOCHS)\n",
    "\n",
    "plt.figure(figsize=(15, 15))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Treinamento')\n",
    "plt.plot(epochs_range, val_acc, label='Validação')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Acurácia de Treinamento e Validação (1)')\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Treinamento')\n",
    "plt.plot(epochs_range, val_loss, label='Validação')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Erro de Treinamento e Validação (2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00012-b7aae2de-c3fd-4cef-8134-c320a8a1ab65",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "# score = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model(f'../model/trained_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predictions = model.predict_classes(X_test)\n",
    "\n",
    "categories = [x.split('/')[-1].split('.')[0] for x in data_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusão\n",
    "\n",
    "A partir da matriz de confusão gerada com as predições dos dados de teste, pode-se perceber que a média de acurácia é aproximadamente 90%, contudo há algumas classes que se distanciam desse valor. Por exemplo, a classe `bus` acurácia mais baixa que a média e pode-se perceber que há muitos falsos positivos, ou seja, o modelo acha que é um ônibus, mas na verdade não é, e a maioria desses falsos positivos são ambulâncias, o que é bem plausível. Esse mesmo fenômeno ocorre ao contrário. Além disso, isso ocorre com pássaros e aviões e também calendários e calculadoras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_test, predictions, normalize='pred')\n",
    "cmd = ConfusionMatrixDisplay(cm, display_labels=categories)\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "cmd.plot(ax=ax, xticks_rotation='vertical', values_format = '.2f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por fim, obtivemos os dados de precision, recall e f1 score. A acurácia final do modelo foi de `88%`. Com esses dados, podemos ver que a classe `beach` possui um precision bem menor que o recall, o que pode ser comprovado na matriz de confusão. O eixo vertical dessa classe(tirando a predição certa) representa os falsos positivos, já o horizontal os falsos negativos. Como o precision é menor a soma desses valores do eixo vertical é maior do que as do eixo horizontal. Além disso, esses valores que representam os falsos positivos estão muito espalhados, o modelo acha que muitas categorias são `beach`, quando não são."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00014-d1665cad-bb47-4594-a5b0-e7e64469a6c1",
    "deepnote_cell_type": "code"
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, predictions, target_names = categories)) "
   ]
  }
 ],
 "metadata": {
  "deepnote": {},
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "c7f01c0c-2dc0-4900-a261-ef3e47813189",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
